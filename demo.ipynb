{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e89edb68-ed02-4c2a-884e-254e8291ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface ASR dataset to be tested\n",
    "DATASET = 'google/fleurs'\n",
    "LANGUAGE = 'en_us'\n",
    "SPLIT = 'test'\n",
    "\n",
    "# Whisper model name, can be one of the following: tiny/tiny.en/base/base.en/small/small.en/medium/medium.en\n",
    "WHISPER_MODEL = 'tiny'\n",
    "\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e923c71-af64-4226-a69d-b188cf0efdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained OpenAI Whisper model\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model(WHISPER_MODEL).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51258184-1972-4182-84cd-5586c56dcf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
       "    num_rows: 647\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically download and load Huggingface dataset\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(DATASET, LANGUAGE, split=SPLIT, trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba8720ca-3afe-4d44-8e58-eaf872bd1dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alex/.cache/huggingface/datasets/downloads/extracted/c8b58fb848e0638bab104a45c19a62463a9fbf7005d94f293757657330ac57c6/test/13896470903695476292.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/alex/.cache/huggingface/datasets/downloads/extracted/c8b58fb848e0638bab104a45c19a62463a9fbf7005d94f293757657330ac57c6/test/13896470903695476292.wav'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a random sample from the testset and print the reference \n",
    "import random, os\n",
    "random.seed(100)\n",
    "idx = random.randint(0, len(dataset)-1)\n",
    "\n",
    "# Prepare the path of the selected utterance\n",
    "print(os.path.join(os.path.dirname(dataset[idx]['path']), dataset[idx]['audio']['path']))\n",
    "audio_path = os.path.join(os.path.dirname(dataset[idx]['path']), dataset[idx]['audio']['path'])\n",
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e218fee7-98e5-47c0-958b-8fe8ff677374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR reference: \"sleep interruption is the process of purposefully awakening during your normal sleep period and falling asleep a short time later 10–60 minutes\"\n",
      "ASR reference (raw): \"sleep interruption is the process of purposefully awakening during your normal sleep period and falling asleep a short time later (10–60 minutes).\"\n"
     ]
    }
   ],
   "source": [
    "# The reference transcription for the utterance\n",
    "print(f'ASR reference: \"{dataset[idx][\"transcription\"]}\"')\n",
    "print(f'ASR reference (raw): \"{dataset[idx][\"raw_transcription\"].lower()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b6c91f2-2450-4094-be1f-2a47ba100479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the utterance\n",
    "from whisper.audio import load_audio\n",
    "\n",
    "audio = load_audio(audio_path)\n",
    "audio = torch.from_numpy(audio).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2225abce-c2dc-4b0a-82b5-dade499fe9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Whisper transcription: \" Sleep interruption is the process of purposely waking them during your normal sleep period and falling asleep a short time later. 10 to 16 minutes.\"\n"
     ]
    }
   ],
   "source": [
    "# Whisepr transcription for the original speech signal\n",
    "print(f'Default Whisper transcription: \"{model.transcribe(audio)[\"text\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22cfee35-292a-47d9-b273-73d187cf0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant universal acoustic adversarial attack segment (0.64 seconds in length)\n",
    "import numpy as np\n",
    "\n",
    "loaded_array = np.load(f'audio_attack_segments/{WHISPER_MODEL}.np.npy')\n",
    "audio_attack_segment = torch.from_numpy(loaded_array).to(audio.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3a26504-a501-4a67-9e10-6663b9d1c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend the learned universal attack segment to the original speech signal\n",
    "audio_with_prompts = torch.cat((audio_attack_segment, audio), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b88e8ac-83fe-4aab-b374-cee7968eb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper transcription with the learned attack segment: \"\"\n"
     ]
    }
   ],
   "source": [
    "# Whisepr transcription for the concatenated speech signal\n",
    "# Whisper is \"muted\" in this case\n",
    "print(f'Whisper transcription with the learned attack segment: \"{model.transcribe(audio_with_prompts)[\"text\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ff202fe-d91c-473d-ac0c-6e4d1b4b2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping audio to wav files\n",
    "\n",
    "import shutil\n",
    "from scipy.io import wavfile\n",
    "\n",
    "shutil.copyfile(audio_path, \"original_audio.wav\")\n",
    "audio_attack_array = audio_with_prompts.cpu().numpy()\n",
    "audio_attack_array = (audio_attack_array * 32767).astype(np.int16)\n",
    "wavfile.write('audio_with_attack.wav', 16000, audio_attack_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7f650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
